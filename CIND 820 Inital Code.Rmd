---
title: "CIND 820 Initial Code and Results"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
<center> <h1> Capstone (10%) </h1> </center>
<center>  <h3> Tejpreet Bhatti </h2> </center>
<center> <h3> DM0 & 500822071 </h2> </center>
---

```{r}
# Loading packages

#Packages for feature selection and decision tree
#install.packages('rlang')
#library(rlang)
#library(Boruta)
#library(rapportools)
#library(tidyverse)
#library(ggplot2)
#library(RColorBrewer)
#library(gridExtra)
#library(corrplot)
#library(corrgram)
#library(arules)
#library(arulesViz)
#library(dplyr)
#library(caret)
#library(dplyr)
#library(rpart)
#library(rattle)
#library(mlbench)

#library(ggraph)
#library(igraph)
#require(tidyverse)
#library(factoextra)
#library(cluster)
#library(purrr)

#Knn packages
#library(class)
#library(e1071)
#library(FNN) 
#library(gmodels) 
#library(psych)

# Naive Bayes packages
#library(naivebayes)
#library(klaR)
#library(MLmetrics)
```

```{r}

# Reading CSV file
mydata1<-read.csv("C:\\Users\\tejpr\\Downloads\\WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv")

names(mydata1) <- gsub("_", "", names(mydata1))

summary(mydata1)

mydata1[mydata1 == ""] <- NA

mydata1 <- mydata1[complete.cases(mydata1),]

#Binning the clv into 3 classes
mydata1$CLV_Levels <- cut(mydata1$Customer.Lifetime.Value, 
                         breaks = c(0,5000,8000,Inf), 
                         labels = c("Low_CLV","AVG_CLV","High_CLV"))

str(mydata1)

# Getting rid of unnecessary variables 
mydata2 <- mydata1[,-3]
mydata3 <- mydata2[,-1]
mydata4 <- mydata3[,-5]

# Converting categorical variables to factors variables 
mydata4$State <- as.factor(mydata4$State)
mydata4$Response <- as.factor(mydata4$Response)
mydata4$Coverage <- as.factor(mydata4$Coverage)
mydata4$Education <- as.factor(mydata4$Education)
mydata4$EmploymentStatus <- as.factor(mydata4$EmploymentStatus)
mydata4$Gender <- as.factor(mydata4$Gender)
mydata4$Location.Code <- as.factor(mydata4$Location.Code)
mydata4$Marital.Status <- as.factor(mydata4$Marital.Status)
mydata4$Policy.Type <- as.factor(mydata4$Policy.Type)
mydata4$Policy <- as.factor(mydata4$Policy)
mydata4$Renew.Offer.Type <- as.factor(mydata4$Renew.Offer.Type)
mydata4$Sales.Channel <- as.factor(mydata4$Sales.Channel)
mydata4$Vehicle.Class <- as.factor(mydata4$Vehicle.Class)
mydata4$Vehicle.Size <- as.factor(mydata4$Vehicle.Size)

# Converting factor variables to numeric variables 
mydata4$State <- as.numeric(mydata4$State)
mydata4$Response <- as.numeric(mydata4$Response)
mydata4$Coverage <- as.numeric(mydata4$Coverage)
mydata4$Education <- as.numeric(mydata4$Education)
mydata4$EmploymentStatus <- as.numeric(mydata4$EmploymentStatus)
mydata4$Gender <- as.numeric(mydata4$Gender)
mydata4$Location.Code <- as.numeric(mydata4$Location.Code)
mydata4$Marital.Status <- as.numeric(mydata4$Marital.Status)
mydata4$Policy.Type <- as.numeric(mydata4$Policy.Type)
mydata4$Policy <- as.numeric(mydata4$Policy)
mydata4$Renew.Offer.Type <- as.numeric(mydata4$Renew.Offer.Type)
mydata4$Sales.Channel <- as.numeric(mydata4$Sales.Channel)
mydata4$Vehicle.Class <- as.numeric(mydata4$Vehicle.Class)
mydata4$Vehicle.Size <- as.numeric(mydata4$Vehicle.Size)

str(mydata4)
```

```{r}
#Boruta
library(Boruta)


#Training and setting seed. implementing and checking the performance of boruta package
set.seed(123)
boruta.train <- Boruta(mydata4,mydata4$CLV_Levels, doTrace = 2)
print(boruta.train)

#plotting the boruta variable importance chart.
plot(boruta.train, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta.train$ImpHistory),function(i)boruta.train$ImpHistory[is.finite(boruta.train$ImpHistory[,i]),i])
names(lz) <- colnames(boruta.train$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),at = 1:ncol(boruta.train$ImpHistory), cex.axis = 0.7)

#list of confirmed attributes
getSelectedAttributes(boruta.train, withTentative = F)

#data frame of the final result derived from Boruta.
boruta.df <- attStats(boruta.train)
class(boruta.df)
print(boruta.df)
```

```{r}
#variable importance
library(caret)
library(rpart)

# Train an rpart model and compute variable importance.
set.seed(100)
rPartMod <- train(CLV_Levels ~ ., data=mydata4, method="rpart")
rpartImp <- varImp(rPartMod)
print(rpartImp)

#Plotting variable importance
plot(rpartImp, top = 20, main='Variable Importance')

```

```{r}
#Recursive feature elimination 

#This feature section method is fully functioning but it takes many hours to process on my #computer. So your welcome to get rid of the hastags and run it to confirm its functionality.

# spliting the dataset into a training and testing set and split data into training (80%) and testing set (20%)

#parts = createDataPartition(mydata4$CLV_Levels, p = .8, list = F)
#train = mydata4[parts, ]
#test = mydata4[-parts, ]
#X_train = train[,-22]
#y_train = train[,22]

#We will use rfe() function from CARET package to implement Recursive Feature elimination.
# Specifying the Cross-validation technique
#control_rfe = rfeControl(functions = rfFuncs, # random forest
#                         method = "repeatedcv", # repeated cv
#                         repeats = 3, # number of repeats
#                         number = 10) # number of folds

#set.seed(50)
# Performing RFE
#result_rfe = rfe(x = X_train, 
#                 y = y_train, 
#                 sizes = c(1:21),
#                 rfeControl = control_rfe)

# summarising the results
#result_rfe
```

```{r}
#Final dataset used for models


mydata5 <- mydata4[,c(1,3,4,5,6,7,8,9,10,14,15,16,18,20,21,22)]

#preproc <- preProcess(mydata5[,c(6,9,10)], method=c("range"))
 
#mydata6 <- predict(preproc, mydata5[,c(6,9,10)])

#data normalization for numeric variables
preproc <- preProcess(mydata5[,c(1:15)], method=c("range"))

mydata6 <- predict(preproc, mydata5[,c(1:15)])
 
summary(mydata6)

#mydata7 <- mydata5[,c(1,2,3,4,5,7,8,11,12,13,14,15,16)]

# Target variabl of CLV 
CLV_Levels <- mydata5[,c(16)]

# Combining the numeric and target variables in to one data fram
finaldata <- cbind(mydata6, CLV_Levels)

str(finaldata)

```

```{r}
#Decision Tree
library(rattle)

#runtime for decision tree training
start.time.DT.train <- Sys.time()

# Training the model and dataset spillting into train and test sets
set.seed(123)
sample8020 <- sample(nrow(finaldata), floor(0.8 * nrow(finaldata)))
DTtrain8020 <- finaldata[sample8020,]
DTtest8020 <- finaldata[-sample8020,]

# Running the training algorithm
set.seed(123)
DTmodel8020 <- train(CLV_Levels~., data = DTtrain8020 , method = "rpart",
                     trControl = trainControl("cv", number = 10),
                     tuneLength = 15)

end.time.DT.train <- Sys.time()
time.taken.DT.train <- round(end.time.DT.train - start.time.DT.train,2)
time.taken.DT.train

# Plot model accuracy vs different values of clv

graph1<-plot(DTmodel8020, main = "DECISION TREE CLV MODEL", cex.main = 3, cex.lab = 2)

# Print the confusion matrix for the model
confusionMatrix(DTmodel8020)

#runtime for decision tree prediction
start.time.DT.pred <- Sys.time()

# Model prediction on test
set.seed(123)
DTpredict8020 <- predict(DTmodel8020,DTtest8020)

end.time.DT.pred <- Sys.time()
time.taken.DT.pred <- round(end.time.DT.pred - start.time.DT.pred,2)
time.taken.DT.pred

# Confusion Matrix on results
confusionMatrix(DTpredict8020,DTtest8020$CLV_Levels)

# Train the model on the entire dataset for a final model.
set.seed(123)
CLV_model <- rpart(CLV_Levels~.,data = finaldata, method = "class", cp = 0.001984564)

# Plot the model
tree <-fancyRpartPlot(CLV_model, main = "Customer Lifetime Value Model", sub = "", cex = 0.5)
```
```{r}
# Precision and F1 score for decision tree

DTPrecision_Low <- (676/(676+13+0))
DTPrecision_Low

DTPrecision_Avg <- (485/(24+485+34))
DTPrecision_Avg

DTPrecision_High <- (552/(0+43+552))
DTPrecision_High

DTF1_Low <- (2*0.98*0.97)/(0.98+0.97)
DTF1_Low

DTF1_Avg <- (2*0.89*0.90)/(0.89+0.90)
DTF1_Avg

DTF1_High <- (2*0.93*0.94)/(0.93+0.94)
DTF1_High
```


```{r}
#KNN--using this version of KNN for analysis

#library(caret)
library(class)
library(dplyr)
library(e1071)
library(FNN) 
library(gmodels) 
library(psych)

# put outcome in its own object
clv_outcome <- finaldata %>% select(CLV_Levels)

# remove original variable from the data set
data_class <- finaldata %>% select(-CLV_Levels)

smp_size <- sample(1:nrow(data_class), 0.8 * nrow(data_class))

# runtime calculation for KNN training
start.time.KNN.train <- Sys.time()

# Dataset splitting train and test sets (80:20)
set.seed(1234)
smp_size <- floor(0.8 * nrow(data_class))
train_ind <- sample(seq_len(nrow(data_class)), size = smp_size)

class_pred_train <- data_class[train_ind, ]
class_pred_test <- data_class[-train_ind, ]

clv_outcome_train <- clv_outcome[train_ind, ]
clv_outcome_test <- clv_outcome[-train_ind, ]

end.time.KNN.train <- Sys.time()
time.taken.KNN.train <- round(end.time.KNN.train - start.time.KNN.train,2)
time.taken.KNN.train

# runtime calculation for KNN prediction
start.time.KNN.pred <- Sys.time()

# Running the training algorithm
clv_KNN <- knn(class_pred_train,class_pred_test,cl=clv_outcome_train,k=9)

end.time.KNN.pred <- Sys.time()
time.taken.KNN.pred <- round(end.time.KNN.pred - start.time.KNN.pred,2)
time.taken.KNN.pred

# Confusion Matrix on results
confusionMatrix(clv_KNN, clv_outcome_test)
```

```{r}
#Caret KNN

# Running the training algorithm
clv_pred_caret <- train(class_pred_train, clv_outcome_train, method = "knn",trControl=trainControl(method='cv',number=10), preProcess = c("center","scale"))
clv_pred_caret

#Plotting amount of neighbours vs accuracy
plot(clv_pred_caret)

# Running the prediction algorithm
knnPredict <- predict(clv_pred_caret, newdata = class_pred_test) 

# Confusion Matrix on results
confusionMatrix(knnPredict, clv_outcome_test)
```
```{r}
# Precision and F1 score for KNN

KNNPrecision_Low <- (46/(461+115+155))
KNNPrecision_Low

KNNPrecision_Avg <- (318/(87+318+84))
KNNPrecision_Avg

KNNPrecision_High <- (355/(142+110+355))
KNNPrecision_High

KNNF1_Low <- (2*0.63*0.67)/(0.63+0.67)
KNNF1_Low

KNNF1_Avg <- (2*0.65*0.59)/(0.65+0.59)
KNNF1_Avg

KNNF1_High <- (2*0.58*0.60)/(0.58+0.60)
KNNF1_High
```


```{r}
#Naive Bayes

#library(naivebayes)
#library(klaR)
#library(MLmetrics)

y = finaldata$CLV_Levels

# runtime calculation for Naive Bayes training
start.time.NB.train <- Sys.time()

# Dataset splitting train and test sets (80:20)
set.seed(1234)
NBsample8020 <- sample(nrow(finaldata), floor(0.8 * nrow(finaldata)))
NBtrain8020 <- finaldata[NBsample8020,]
NBtest8020 <- finaldata[-NBsample8020,]

# Running the training algorithm
NBmodel = train(finaldata,y,'nb',trControl=trainControl(method='cv',number=10))
NBmodel

end.time.NB.train <- Sys.time()
time.taken.NB.train <- round(end.time.NB.train - start.time.NB.train,2)
time.taken.NB.train

# Print the confusion matrix for the model
confusionMatrix(NBmodel)

# runtime calculation for Naive Bayes prediction
start.time.NB.pred <- Sys.time()

# Model prediction on test
set.seed(1234)
NBpredict8020 <- predict(NBmodel,NBtest8020)

end.time.NB.pred <- Sys.time()
time.taken.NB.pred <- round(end.time.NB.pred - start.time.NB.pred,2)
time.taken.NB.pred

# Confusion Matrix on results
confusionMatrix(NBpredict8020,NBtest8020$CLV_Levels)
```

```{r}
#Random Forest--Not complete


#install.packages("MLmetrics")
#library(MLmetrics)

#set.seed(1234)
#partition <- createDataPartition(y = mydata5$CLV_Levels, p = 0.8, list = FALSE)
#rf.train <- mydata5[partition, ]
#rf.test <- mydata5[-partition, ]
#rm(partition)

#auto.frst = train(CLV_Levels ~ ., 
#                data = rf.train, 
#                method = "ranger",  # for random forest
#                tuneLength = 5,  # choose up to 5 combinations of tuning parameters
#                metric = "Accuracy",  # evaluate hyperparamter combinations with ROC
#                trControl = trainControl(
#                  method = "cv",  # k-fold cross validation
#                  number = 10,  # 10 folds
#                )
#)
#auto.frst

#plot(auto.frst)

#rf.pred <- predict(auto.frst, rf.test, type = "raw")
#plot(oj.test$Purchase, rf.pred, 
#     main = "Random Forest Classification: Predicted vs. Actual",
#     xlab = "Actual",
#     ylab = "Predicted")

#(oj.conf <- confusionMatrix(data = rf.pred, reference = rf.test$CLV_Levels))
```

